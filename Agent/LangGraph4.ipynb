{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Failed",
   "id": "c1f1be02ae37a101"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-03T08:56:25.287330Z",
     "start_time": "2024-06-03T08:56:25.259754Z"
    }
   },
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T09:32:50.563831Z",
     "start_time": "2024-06-03T09:32:50.555901Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.messages import BaseMessage, ToolMessage, AIMessage, HumanMessage\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langchain.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n"
   ],
   "id": "bf46e955cc1acf07",
   "outputs": [],
   "execution_count": 74
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T09:32:50.739706Z",
     "start_time": "2024-06-03T09:32:50.735085Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class State(TypedDict):\n",
    "    messages: list[BaseMessage]"
   ],
   "id": "201ba71f10dce83f",
   "outputs": [],
   "execution_count": 75
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T09:32:50.902399Z",
     "start_time": "2024-06-03T09:32:50.896446Z"
    }
   },
   "cell_type": "code",
   "source": "internet_tool = TavilySearchResults(max_results=2)",
   "id": "3738425e360f7642",
   "outputs": [],
   "execution_count": 76
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T09:32:52.585840Z",
     "start_time": "2024-06-03T09:32:51.046340Z"
    }
   },
   "cell_type": "code",
   "source": [
    "embedding_function = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "db = Chroma(persist_directory=\"../Private/chroma_db\", embedding_function=embedding_function)\n",
    "retriever = db.as_retriever()\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever=retriever,\n",
    "    name=\"retrieve_story_script\",\n",
    "    description=\"Search and return information about the story script.\"\n",
    ")"
   ],
   "id": "2b9fa14a31c616e3",
   "outputs": [],
   "execution_count": 77
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T09:32:52.594304Z",
     "start_time": "2024-06-03T09:32:52.589316Z"
    }
   },
   "cell_type": "code",
   "source": "tools = [internet_tool, retriever_tool]",
   "id": "35d312d6ed69d4b9",
   "outputs": [],
   "execution_count": 78
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T09:32:54.581323Z",
     "start_time": "2024-06-03T09:32:52.595840Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = ChatOpenAI(model=\"gpt-4o\", temperature=1.0)\n",
    "model_with_tools = model.bind_tools(tools=tools)"
   ],
   "id": "be00594f8bb5e691",
   "outputs": [],
   "execution_count": 79
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T09:35:47.845776Z",
     "start_time": "2024-06-03T09:35:47.840585Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "     \"\"\"\n",
    "    You are Emi Ibarazaki from Katawa Shoujo.\n",
    "    I am Hisao.\n",
    "    You are my girlfriend. \n",
    "    I want you to respond and answer like Emi using the tone, manner and vocabulary Emi would use. \n",
    "    Do not write any explanations. Only answer like Emi.\n",
    "\n",
    "    You study in Yamaku Academy, Sendai.\n",
    "     \"\"\"),\n",
    "    MessagesPlaceholder(variable_name=\"messages\")\n",
    "])\n",
    "\n",
    "chain = prompt | model_with_tools"
   ],
   "id": "2dd6e2869ca471d8",
   "outputs": [],
   "execution_count": 87
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T09:35:48.015500Z",
     "start_time": "2024-06-03T09:35:47.999795Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def chatbot(state: State):\n",
    "    chatbot_outcome = chain.invoke(state[\"messages\"])\n",
    "    state[\"messages\"].append(chatbot_outcome)\n",
    "    return {\"messages\": state[\"messages\"]}\n",
    "\n",
    "def reduce_msg_history(state: State):\n",
    "    reduced_list = [m for m in state[\"messages\"] if not isinstance(m, ToolMessage)]     \n",
    "    return {\"messages\": reduced_list}"
   ],
   "id": "10bcb3f566c8caa2",
   "outputs": [],
   "execution_count": 88
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T09:35:48.144150Z",
     "start_time": "2024-06-03T09:35:48.129740Z"
    }
   },
   "cell_type": "code",
   "source": [
    "graph_builder = StateGraph(state_schema=State)\n",
    "memory = SqliteSaver.from_conn_string(\":memory:\")\n",
    "\n",
    "graph_builder.add_node(\"reduce_msg_history\", reduce_msg_history)\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "graph_builder.add_edge(\"reduce_msg_history\", \"chatbot\")\n",
    "tool_node = ToolNode(tools=tools)\n",
    "graph_builder.add_node(\"tools\", tool_node)\n",
    "graph_builder.add_conditional_edges(\"chatbot\", tools_condition)\n",
    "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
    "graph_builder.set_entry_point(\"reduce_msg_history\")\n",
    "graph = graph_builder.compile(checkpointer=memory,)"
   ],
   "id": "b518a8fbb9a7632e",
   "outputs": [],
   "execution_count": 89
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T09:35:49.031580Z",
     "start_time": "2024-06-03T09:35:48.259457Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from IPython.display import Image, display\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ],
   "id": "e8fafe5063c9d559",
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 90
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T09:35:49.038642Z",
     "start_time": "2024-06-03T09:35:49.033604Z"
    }
   },
   "cell_type": "code",
   "source": "config = {\"configurable\": {\"thread_id\": \"1\"}}",
   "id": "ba301b73eb75c377",
   "outputs": [],
   "execution_count": 91
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T09:35:49.048432Z",
     "start_time": "2024-06-03T09:35:49.039669Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def conversation(config):\n",
    "\n",
    "    user_input = input()\n",
    "\n",
    "    events = graph.stream(\n",
    "        {\"messages\": [HumanMessage(content=user_input)]}, config, stream_mode=\"values\"\n",
    "    )\n",
    "    for event in events:\n",
    "        print(event)\n",
    "        # event[\"messages\"][-1].pretty_print()"
   ],
   "id": "10481b39939d586a",
   "outputs": [],
   "execution_count": 92
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T09:40:02.285887Z",
     "start_time": "2024-06-03T09:39:48.672898Z"
    }
   },
   "cell_type": "code",
   "source": "conversation(config)",
   "id": "306be0f915bd0cfd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='Can you get the weather today')]}\n",
      "{'messages': [HumanMessage(content='Can you get the weather today')]}\n",
      "{'messages': [HumanMessage(content='Can you get the weather today'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_u9BSI28isflkIgIZ6F2xBxvV', 'function': {'arguments': '{ \"query\": \"current weather in Sendai today\" }', 'name': 'tavily_search_results_json'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 193, 'total_tokens': 216}, 'model_name': 'gpt-4o', 'system_fingerprint': 'fp_319be4768e', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-50c5f9fd-e8e5-423c-a8ac-0b23040b988c-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'current weather in Sendai today'}, 'id': 'call_u9BSI28isflkIgIZ6F2xBxvV'}])]}\n",
      "{'messages': [ToolMessage(content='[{\"url\": \"https://www.weatherapi.com/\", \"content\": \"{\\'location\\': {\\'name\\': \\'Sendai\\', \\'region\\': \\'Miyagi\\', \\'country\\': \\'Japan\\', \\'lat\\': 38.26, \\'lon\\': 140.87, \\'tz_id\\': \\'Asia/Tokyo\\', \\'localtime_epoch\\': 1717407566, \\'localtime\\': \\'2024-06-03 18:39\\'}, \\'current\\': {\\'last_updated_epoch\\': 1717407000, \\'last_updated\\': \\'2024-06-03 18:30\\', \\'temp_c\\': 16.0, \\'temp_f\\': 60.8, \\'is_day\\': 1, \\'condition\\': {\\'text\\': \\'Partly cloudy\\', \\'icon\\': \\'//cdn.weatherapi.com/weather/64x64/day/116.png\\', \\'code\\': 1003}, \\'wind_mph\\': 15.0, \\'wind_kph\\': 24.1, \\'wind_degree\\': 350, \\'wind_dir\\': \\'N\\', \\'pressure_mb\\': 1009.0, \\'pressure_in\\': 29.8, \\'precip_mm\\': 0.07, \\'precip_in\\': 0.0, \\'humidity\\': 88, \\'cloud\\': 75, \\'feelslike_c\\': 16.0, \\'feelslike_f\\': 60.8, \\'windchill_c\\': 14.0, \\'windchill_f\\': 57.2, \\'heatindex_c\\': 14.9, \\'heatindex_f\\': 58.9, \\'dewpoint_c\\': 13.4, \\'dewpoint_f\\': 56.0, \\'vis_km\\': 10.0, \\'vis_miles\\': 6.0, \\'uv\\': 3.0, \\'gust_mph\\': 19.5, \\'gust_kph\\': 31.3}}\"}, {\"url\": \"https://www.timeanddate.com/weather/japan/sendai/ext\", \"content\": \"Sendai 14 Day Extended Forecast. Weather Today Weather Hourly 14 Day Forecast Yesterday/Past Weather Climate (Averages) Currently: 61 \\\\u00b0F. Light rain. Fog. (Weather station: Sendai Airport, Japan). See more current weather.\"}]', name='tavily_search_results_json', tool_call_id='call_u9BSI28isflkIgIZ6F2xBxvV')]}\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': \"Invalid parameter: messages with role 'tool' must be a response to a preceeding message with 'tool_calls'.\", 'type': 'invalid_request_error', 'param': 'messages.[1].role', 'code': None}}",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mBadRequestError\u001B[0m                           Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[96], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m conversation(config)\n",
      "Cell \u001B[1;32mIn[92], line 8\u001B[0m, in \u001B[0;36mconversation\u001B[1;34m(config)\u001B[0m\n\u001B[0;32m      3\u001B[0m user_input \u001B[38;5;241m=\u001B[39m \u001B[38;5;28minput\u001B[39m()\n\u001B[0;32m      5\u001B[0m events \u001B[38;5;241m=\u001B[39m graph\u001B[38;5;241m.\u001B[39mstream(\n\u001B[0;32m      6\u001B[0m     {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmessages\u001B[39m\u001B[38;5;124m\"\u001B[39m: [HumanMessage(content\u001B[38;5;241m=\u001B[39muser_input)]}, config, stream_mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvalues\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m      7\u001B[0m )\n\u001B[1;32m----> 8\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m event \u001B[38;5;129;01min\u001B[39;00m events:\n\u001B[0;32m      9\u001B[0m     \u001B[38;5;28mprint\u001B[39m(event)\n",
      "File \u001B[1;32m~\\AppData\\Local\\anaconda3\\envs\\envkhai1\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:949\u001B[0m, in \u001B[0;36mPregel.stream\u001B[1;34m(self, input, config, stream_mode, output_keys, input_keys, interrupt_before, interrupt_after, debug)\u001B[0m\n\u001B[0;32m    946\u001B[0m         \u001B[38;5;28;01mdel\u001B[39;00m fut, task\n\u001B[0;32m    948\u001B[0m \u001B[38;5;66;03m# panic on failure or timeout\u001B[39;00m\n\u001B[1;32m--> 949\u001B[0m _panic_or_proceed(done, inflight, step)\n\u001B[0;32m    950\u001B[0m \u001B[38;5;66;03m# don't keep futures around in memory longer than needed\u001B[39;00m\n\u001B[0;32m    951\u001B[0m \u001B[38;5;28;01mdel\u001B[39;00m done, inflight, futures\n",
      "File \u001B[1;32m~\\AppData\\Local\\anaconda3\\envs\\envkhai1\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:1473\u001B[0m, in \u001B[0;36m_panic_or_proceed\u001B[1;34m(done, inflight, step)\u001B[0m\n\u001B[0;32m   1471\u001B[0m             inflight\u001B[38;5;241m.\u001B[39mpop()\u001B[38;5;241m.\u001B[39mcancel()\n\u001B[0;32m   1472\u001B[0m         \u001B[38;5;66;03m# raise the exception\u001B[39;00m\n\u001B[1;32m-> 1473\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m exc\n\u001B[0;32m   1475\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m inflight:\n\u001B[0;32m   1476\u001B[0m     \u001B[38;5;66;03m# if we got here means we timed out\u001B[39;00m\n\u001B[0;32m   1477\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m inflight:\n\u001B[0;32m   1478\u001B[0m         \u001B[38;5;66;03m# cancel all pending tasks\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\anaconda3\\envs\\envkhai1\\Lib\\concurrent\\futures\\thread.py:58\u001B[0m, in \u001B[0;36m_WorkItem.run\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     55\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[0;32m     57\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 58\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfn(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mkwargs)\n\u001B[0;32m     59\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[0;32m     60\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfuture\u001B[38;5;241m.\u001B[39mset_exception(exc)\n",
      "File \u001B[1;32m~\\AppData\\Local\\anaconda3\\envs\\envkhai1\\Lib\\site-packages\\langgraph\\pregel\\retry.py:66\u001B[0m, in \u001B[0;36mrun_with_retry\u001B[1;34m(task, retry_policy)\u001B[0m\n\u001B[0;32m     64\u001B[0m task\u001B[38;5;241m.\u001B[39mwrites\u001B[38;5;241m.\u001B[39mclear()\n\u001B[0;32m     65\u001B[0m \u001B[38;5;66;03m# run the task\u001B[39;00m\n\u001B[1;32m---> 66\u001B[0m task\u001B[38;5;241m.\u001B[39mproc\u001B[38;5;241m.\u001B[39minvoke(task\u001B[38;5;241m.\u001B[39minput, task\u001B[38;5;241m.\u001B[39mconfig)\n\u001B[0;32m     67\u001B[0m \u001B[38;5;66;03m# if successful, end\u001B[39;00m\n\u001B[0;32m     68\u001B[0m \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\anaconda3\\envs\\envkhai1\\Lib\\site-packages\\langchain_core\\runnables\\base.py:2399\u001B[0m, in \u001B[0;36mRunnableSequence.invoke\u001B[1;34m(self, input, config)\u001B[0m\n\u001B[0;32m   2397\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   2398\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i, step \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msteps):\n\u001B[1;32m-> 2399\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m step\u001B[38;5;241m.\u001B[39minvoke(\n\u001B[0;32m   2400\u001B[0m             \u001B[38;5;28minput\u001B[39m,\n\u001B[0;32m   2401\u001B[0m             \u001B[38;5;66;03m# mark each step as a child run\u001B[39;00m\n\u001B[0;32m   2402\u001B[0m             patch_config(\n\u001B[0;32m   2403\u001B[0m                 config, callbacks\u001B[38;5;241m=\u001B[39mrun_manager\u001B[38;5;241m.\u001B[39mget_child(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mseq:step:\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   2404\u001B[0m             ),\n\u001B[0;32m   2405\u001B[0m         )\n\u001B[0;32m   2406\u001B[0m \u001B[38;5;66;03m# finish the root run\u001B[39;00m\n\u001B[0;32m   2407\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[1;32m~\\AppData\\Local\\anaconda3\\envs\\envkhai1\\Lib\\site-packages\\langgraph\\utils.py:95\u001B[0m, in \u001B[0;36mRunnableCallable.invoke\u001B[1;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[0;32m     93\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m accepts_config(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfunc):\n\u001B[0;32m     94\u001B[0m         kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mconfig\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m config\n\u001B[1;32m---> 95\u001B[0m     ret \u001B[38;5;241m=\u001B[39m context\u001B[38;5;241m.\u001B[39mrun(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfunc, \u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m     96\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(ret, Runnable) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrecurse:\n\u001B[0;32m     97\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m ret\u001B[38;5;241m.\u001B[39minvoke(\u001B[38;5;28minput\u001B[39m, config)\n",
      "Cell \u001B[1;32mIn[88], line 2\u001B[0m, in \u001B[0;36mchatbot\u001B[1;34m(state)\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mchatbot\u001B[39m(state: State):\n\u001B[1;32m----> 2\u001B[0m     chatbot_outcome \u001B[38;5;241m=\u001B[39m chain\u001B[38;5;241m.\u001B[39minvoke(state[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmessages\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[0;32m      3\u001B[0m     state[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmessages\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mappend(chatbot_outcome)\n\u001B[0;32m      4\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmessages\u001B[39m\u001B[38;5;124m\"\u001B[39m: state[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmessages\u001B[39m\u001B[38;5;124m\"\u001B[39m]}\n",
      "File \u001B[1;32m~\\AppData\\Local\\anaconda3\\envs\\envkhai1\\Lib\\site-packages\\langchain_core\\runnables\\base.py:2399\u001B[0m, in \u001B[0;36mRunnableSequence.invoke\u001B[1;34m(self, input, config)\u001B[0m\n\u001B[0;32m   2397\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   2398\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i, step \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msteps):\n\u001B[1;32m-> 2399\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m step\u001B[38;5;241m.\u001B[39minvoke(\n\u001B[0;32m   2400\u001B[0m             \u001B[38;5;28minput\u001B[39m,\n\u001B[0;32m   2401\u001B[0m             \u001B[38;5;66;03m# mark each step as a child run\u001B[39;00m\n\u001B[0;32m   2402\u001B[0m             patch_config(\n\u001B[0;32m   2403\u001B[0m                 config, callbacks\u001B[38;5;241m=\u001B[39mrun_manager\u001B[38;5;241m.\u001B[39mget_child(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mseq:step:\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   2404\u001B[0m             ),\n\u001B[0;32m   2405\u001B[0m         )\n\u001B[0;32m   2406\u001B[0m \u001B[38;5;66;03m# finish the root run\u001B[39;00m\n\u001B[0;32m   2407\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[1;32m~\\AppData\\Local\\anaconda3\\envs\\envkhai1\\Lib\\site-packages\\langchain_core\\runnables\\base.py:4433\u001B[0m, in \u001B[0;36mRunnableBindingBase.invoke\u001B[1;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[0;32m   4427\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minvoke\u001B[39m(\n\u001B[0;32m   4428\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   4429\u001B[0m     \u001B[38;5;28minput\u001B[39m: Input,\n\u001B[0;32m   4430\u001B[0m     config: Optional[RunnableConfig] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   4431\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Optional[Any],\n\u001B[0;32m   4432\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Output:\n\u001B[1;32m-> 4433\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbound\u001B[38;5;241m.\u001B[39minvoke(\n\u001B[0;32m   4434\u001B[0m         \u001B[38;5;28minput\u001B[39m,\n\u001B[0;32m   4435\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_merge_configs(config),\n\u001B[0;32m   4436\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m{\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mkwargs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs},\n\u001B[0;32m   4437\u001B[0m     )\n",
      "File \u001B[1;32m~\\AppData\\Local\\anaconda3\\envs\\envkhai1\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:170\u001B[0m, in \u001B[0;36mBaseChatModel.invoke\u001B[1;34m(self, input, config, stop, **kwargs)\u001B[0m\n\u001B[0;32m    159\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minvoke\u001B[39m(\n\u001B[0;32m    160\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    161\u001B[0m     \u001B[38;5;28minput\u001B[39m: LanguageModelInput,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    165\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[0;32m    166\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m BaseMessage:\n\u001B[0;32m    167\u001B[0m     config \u001B[38;5;241m=\u001B[39m ensure_config(config)\n\u001B[0;32m    168\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(\n\u001B[0;32m    169\u001B[0m         ChatGeneration,\n\u001B[1;32m--> 170\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgenerate_prompt(\n\u001B[0;32m    171\u001B[0m             [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_convert_input(\u001B[38;5;28minput\u001B[39m)],\n\u001B[0;32m    172\u001B[0m             stop\u001B[38;5;241m=\u001B[39mstop,\n\u001B[0;32m    173\u001B[0m             callbacks\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcallbacks\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[0;32m    174\u001B[0m             tags\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtags\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[0;32m    175\u001B[0m             metadata\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmetadata\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[0;32m    176\u001B[0m             run_name\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_name\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[0;32m    177\u001B[0m             run_id\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_id\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[0;32m    178\u001B[0m             \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m    179\u001B[0m         )\u001B[38;5;241m.\u001B[39mgenerations[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m0\u001B[39m],\n\u001B[0;32m    180\u001B[0m     )\u001B[38;5;241m.\u001B[39mmessage\n",
      "File \u001B[1;32m~\\AppData\\Local\\anaconda3\\envs\\envkhai1\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:599\u001B[0m, in \u001B[0;36mBaseChatModel.generate_prompt\u001B[1;34m(self, prompts, stop, callbacks, **kwargs)\u001B[0m\n\u001B[0;32m    591\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mgenerate_prompt\u001B[39m(\n\u001B[0;32m    592\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    593\u001B[0m     prompts: List[PromptValue],\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    596\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[0;32m    597\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m LLMResult:\n\u001B[0;32m    598\u001B[0m     prompt_messages \u001B[38;5;241m=\u001B[39m [p\u001B[38;5;241m.\u001B[39mto_messages() \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m prompts]\n\u001B[1;32m--> 599\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgenerate(prompt_messages, stop\u001B[38;5;241m=\u001B[39mstop, callbacks\u001B[38;5;241m=\u001B[39mcallbacks, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\AppData\\Local\\anaconda3\\envs\\envkhai1\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:456\u001B[0m, in \u001B[0;36mBaseChatModel.generate\u001B[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001B[0m\n\u001B[0;32m    454\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m run_managers:\n\u001B[0;32m    455\u001B[0m             run_managers[i]\u001B[38;5;241m.\u001B[39mon_llm_error(e, response\u001B[38;5;241m=\u001B[39mLLMResult(generations\u001B[38;5;241m=\u001B[39m[]))\n\u001B[1;32m--> 456\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[0;32m    457\u001B[0m flattened_outputs \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m    458\u001B[0m     LLMResult(generations\u001B[38;5;241m=\u001B[39m[res\u001B[38;5;241m.\u001B[39mgenerations], llm_output\u001B[38;5;241m=\u001B[39mres\u001B[38;5;241m.\u001B[39mllm_output)  \u001B[38;5;66;03m# type: ignore[list-item]\u001B[39;00m\n\u001B[0;32m    459\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m res \u001B[38;5;129;01min\u001B[39;00m results\n\u001B[0;32m    460\u001B[0m ]\n\u001B[0;32m    461\u001B[0m llm_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_combine_llm_outputs([res\u001B[38;5;241m.\u001B[39mllm_output \u001B[38;5;28;01mfor\u001B[39;00m res \u001B[38;5;129;01min\u001B[39;00m results])\n",
      "File \u001B[1;32m~\\AppData\\Local\\anaconda3\\envs\\envkhai1\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:446\u001B[0m, in \u001B[0;36mBaseChatModel.generate\u001B[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001B[0m\n\u001B[0;32m    443\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(messages):\n\u001B[0;32m    444\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    445\u001B[0m         results\u001B[38;5;241m.\u001B[39mappend(\n\u001B[1;32m--> 446\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate_with_cache(\n\u001B[0;32m    447\u001B[0m                 m,\n\u001B[0;32m    448\u001B[0m                 stop\u001B[38;5;241m=\u001B[39mstop,\n\u001B[0;32m    449\u001B[0m                 run_manager\u001B[38;5;241m=\u001B[39mrun_managers[i] \u001B[38;5;28;01mif\u001B[39;00m run_managers \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    450\u001B[0m                 \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m    451\u001B[0m             )\n\u001B[0;32m    452\u001B[0m         )\n\u001B[0;32m    453\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    454\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m run_managers:\n",
      "File \u001B[1;32m~\\AppData\\Local\\anaconda3\\envs\\envkhai1\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:671\u001B[0m, in \u001B[0;36mBaseChatModel._generate_with_cache\u001B[1;34m(self, messages, stop, run_manager, **kwargs)\u001B[0m\n\u001B[0;32m    669\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    670\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m inspect\u001B[38;5;241m.\u001B[39msignature(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate)\u001B[38;5;241m.\u001B[39mparameters\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_manager\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m--> 671\u001B[0m         result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate(\n\u001B[0;32m    672\u001B[0m             messages, stop\u001B[38;5;241m=\u001B[39mstop, run_manager\u001B[38;5;241m=\u001B[39mrun_manager, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[0;32m    673\u001B[0m         )\n\u001B[0;32m    674\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    675\u001B[0m         result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate(messages, stop\u001B[38;5;241m=\u001B[39mstop, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\AppData\\Local\\anaconda3\\envs\\envkhai1\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:522\u001B[0m, in \u001B[0;36mBaseChatOpenAI._generate\u001B[1;34m(self, messages, stop, run_manager, **kwargs)\u001B[0m\n\u001B[0;32m    520\u001B[0m message_dicts, params \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_message_dicts(messages, stop)\n\u001B[0;32m    521\u001B[0m params \u001B[38;5;241m=\u001B[39m {\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs}\n\u001B[1;32m--> 522\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mcreate(messages\u001B[38;5;241m=\u001B[39mmessage_dicts, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams)\n\u001B[0;32m    523\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_chat_result(response)\n",
      "File \u001B[1;32m~\\AppData\\Local\\anaconda3\\envs\\envkhai1\\Lib\\site-packages\\openai\\_utils\\_utils.py:277\u001B[0m, in \u001B[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    275\u001B[0m             msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMissing required argument: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mquote(missing[\u001B[38;5;241m0\u001B[39m])\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    276\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(msg)\n\u001B[1;32m--> 277\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\AppData\\Local\\anaconda3\\envs\\envkhai1\\Lib\\site-packages\\openai\\resources\\chat\\completions.py:590\u001B[0m, in \u001B[0;36mCompletions.create\u001B[1;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001B[0m\n\u001B[0;32m    558\u001B[0m \u001B[38;5;129m@required_args\u001B[39m([\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmessages\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m], [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmessages\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstream\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[0;32m    559\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcreate\u001B[39m(\n\u001B[0;32m    560\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    588\u001B[0m     timeout: \u001B[38;5;28mfloat\u001B[39m \u001B[38;5;241m|\u001B[39m httpx\u001B[38;5;241m.\u001B[39mTimeout \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m|\u001B[39m NotGiven \u001B[38;5;241m=\u001B[39m NOT_GIVEN,\n\u001B[0;32m    589\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ChatCompletion \u001B[38;5;241m|\u001B[39m Stream[ChatCompletionChunk]:\n\u001B[1;32m--> 590\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_post(\n\u001B[0;32m    591\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/chat/completions\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    592\u001B[0m         body\u001B[38;5;241m=\u001B[39mmaybe_transform(\n\u001B[0;32m    593\u001B[0m             {\n\u001B[0;32m    594\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmessages\u001B[39m\u001B[38;5;124m\"\u001B[39m: messages,\n\u001B[0;32m    595\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m: model,\n\u001B[0;32m    596\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfrequency_penalty\u001B[39m\u001B[38;5;124m\"\u001B[39m: frequency_penalty,\n\u001B[0;32m    597\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfunction_call\u001B[39m\u001B[38;5;124m\"\u001B[39m: function_call,\n\u001B[0;32m    598\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfunctions\u001B[39m\u001B[38;5;124m\"\u001B[39m: functions,\n\u001B[0;32m    599\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlogit_bias\u001B[39m\u001B[38;5;124m\"\u001B[39m: logit_bias,\n\u001B[0;32m    600\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlogprobs\u001B[39m\u001B[38;5;124m\"\u001B[39m: logprobs,\n\u001B[0;32m    601\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmax_tokens\u001B[39m\u001B[38;5;124m\"\u001B[39m: max_tokens,\n\u001B[0;32m    602\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mn\u001B[39m\u001B[38;5;124m\"\u001B[39m: n,\n\u001B[0;32m    603\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpresence_penalty\u001B[39m\u001B[38;5;124m\"\u001B[39m: presence_penalty,\n\u001B[0;32m    604\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mresponse_format\u001B[39m\u001B[38;5;124m\"\u001B[39m: response_format,\n\u001B[0;32m    605\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mseed\u001B[39m\u001B[38;5;124m\"\u001B[39m: seed,\n\u001B[0;32m    606\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstop\u001B[39m\u001B[38;5;124m\"\u001B[39m: stop,\n\u001B[0;32m    607\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstream\u001B[39m\u001B[38;5;124m\"\u001B[39m: stream,\n\u001B[0;32m    608\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstream_options\u001B[39m\u001B[38;5;124m\"\u001B[39m: stream_options,\n\u001B[0;32m    609\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtemperature\u001B[39m\u001B[38;5;124m\"\u001B[39m: temperature,\n\u001B[0;32m    610\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtool_choice\u001B[39m\u001B[38;5;124m\"\u001B[39m: tool_choice,\n\u001B[0;32m    611\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtools\u001B[39m\u001B[38;5;124m\"\u001B[39m: tools,\n\u001B[0;32m    612\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtop_logprobs\u001B[39m\u001B[38;5;124m\"\u001B[39m: top_logprobs,\n\u001B[0;32m    613\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtop_p\u001B[39m\u001B[38;5;124m\"\u001B[39m: top_p,\n\u001B[0;32m    614\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124muser\u001B[39m\u001B[38;5;124m\"\u001B[39m: user,\n\u001B[0;32m    615\u001B[0m             },\n\u001B[0;32m    616\u001B[0m             completion_create_params\u001B[38;5;241m.\u001B[39mCompletionCreateParams,\n\u001B[0;32m    617\u001B[0m         ),\n\u001B[0;32m    618\u001B[0m         options\u001B[38;5;241m=\u001B[39mmake_request_options(\n\u001B[0;32m    619\u001B[0m             extra_headers\u001B[38;5;241m=\u001B[39mextra_headers, extra_query\u001B[38;5;241m=\u001B[39mextra_query, extra_body\u001B[38;5;241m=\u001B[39mextra_body, timeout\u001B[38;5;241m=\u001B[39mtimeout\n\u001B[0;32m    620\u001B[0m         ),\n\u001B[0;32m    621\u001B[0m         cast_to\u001B[38;5;241m=\u001B[39mChatCompletion,\n\u001B[0;32m    622\u001B[0m         stream\u001B[38;5;241m=\u001B[39mstream \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m    623\u001B[0m         stream_cls\u001B[38;5;241m=\u001B[39mStream[ChatCompletionChunk],\n\u001B[0;32m    624\u001B[0m     )\n",
      "File \u001B[1;32m~\\AppData\\Local\\anaconda3\\envs\\envkhai1\\Lib\\site-packages\\openai\\_base_client.py:1240\u001B[0m, in \u001B[0;36mSyncAPIClient.post\u001B[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001B[0m\n\u001B[0;32m   1226\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpost\u001B[39m(\n\u001B[0;32m   1227\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   1228\u001B[0m     path: \u001B[38;5;28mstr\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1235\u001B[0m     stream_cls: \u001B[38;5;28mtype\u001B[39m[_StreamT] \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   1236\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ResponseT \u001B[38;5;241m|\u001B[39m _StreamT:\n\u001B[0;32m   1237\u001B[0m     opts \u001B[38;5;241m=\u001B[39m FinalRequestOptions\u001B[38;5;241m.\u001B[39mconstruct(\n\u001B[0;32m   1238\u001B[0m         method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpost\u001B[39m\u001B[38;5;124m\"\u001B[39m, url\u001B[38;5;241m=\u001B[39mpath, json_data\u001B[38;5;241m=\u001B[39mbody, files\u001B[38;5;241m=\u001B[39mto_httpx_files(files), \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions\n\u001B[0;32m   1239\u001B[0m     )\n\u001B[1;32m-> 1240\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(ResponseT, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrequest(cast_to, opts, stream\u001B[38;5;241m=\u001B[39mstream, stream_cls\u001B[38;5;241m=\u001B[39mstream_cls))\n",
      "File \u001B[1;32m~\\AppData\\Local\\anaconda3\\envs\\envkhai1\\Lib\\site-packages\\openai\\_base_client.py:921\u001B[0m, in \u001B[0;36mSyncAPIClient.request\u001B[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001B[0m\n\u001B[0;32m    912\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrequest\u001B[39m(\n\u001B[0;32m    913\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    914\u001B[0m     cast_to: Type[ResponseT],\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    919\u001B[0m     stream_cls: \u001B[38;5;28mtype\u001B[39m[_StreamT] \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    920\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ResponseT \u001B[38;5;241m|\u001B[39m _StreamT:\n\u001B[1;32m--> 921\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_request(\n\u001B[0;32m    922\u001B[0m         cast_to\u001B[38;5;241m=\u001B[39mcast_to,\n\u001B[0;32m    923\u001B[0m         options\u001B[38;5;241m=\u001B[39moptions,\n\u001B[0;32m    924\u001B[0m         stream\u001B[38;5;241m=\u001B[39mstream,\n\u001B[0;32m    925\u001B[0m         stream_cls\u001B[38;5;241m=\u001B[39mstream_cls,\n\u001B[0;32m    926\u001B[0m         remaining_retries\u001B[38;5;241m=\u001B[39mremaining_retries,\n\u001B[0;32m    927\u001B[0m     )\n",
      "File \u001B[1;32m~\\AppData\\Local\\anaconda3\\envs\\envkhai1\\Lib\\site-packages\\openai\\_base_client.py:1020\u001B[0m, in \u001B[0;36mSyncAPIClient._request\u001B[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001B[0m\n\u001B[0;32m   1017\u001B[0m         err\u001B[38;5;241m.\u001B[39mresponse\u001B[38;5;241m.\u001B[39mread()\n\u001B[0;32m   1019\u001B[0m     log\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRe-raising status error\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m-> 1020\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_make_status_error_from_response(err\u001B[38;5;241m.\u001B[39mresponse) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1022\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_process_response(\n\u001B[0;32m   1023\u001B[0m     cast_to\u001B[38;5;241m=\u001B[39mcast_to,\n\u001B[0;32m   1024\u001B[0m     options\u001B[38;5;241m=\u001B[39moptions,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1027\u001B[0m     stream_cls\u001B[38;5;241m=\u001B[39mstream_cls,\n\u001B[0;32m   1028\u001B[0m )\n",
      "\u001B[1;31mBadRequestError\u001B[0m: Error code: 400 - {'error': {'message': \"Invalid parameter: messages with role 'tool' must be a response to a preceeding message with 'tool_calls'.\", 'type': 'invalid_request_error', 'param': 'messages.[1].role', 'code': None}}"
     ]
    }
   ],
   "execution_count": 96
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T09:31:11.751344Z",
     "start_time": "2024-06-03T09:31:11.745823Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "622dc8c3cf97d7b2",
   "outputs": [],
   "execution_count": 73
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e09db87f2e2457f7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
